

# Articleâ€¯Internalâ€¯Linkâ€¯Builder

A web application that automates generating contextually relevant internal links for blog articles on your site. Crawl your site (custom HTML or WordPress), extract metadata from blog URLs, crossâ€‘reference with your article content using AI (via Googleâ€¯Geminiâ€¯API), and insert or export internal links for improved SEO and content discoverability.

---

## ğŸŒŸ Features

* Website Crawling: Enter a custom site URL or a WordPress site; the crawler (via sitemap or recursive crawl) will discover blog URLs.
* Metadata Extraction: For each URL it finds, it pulls meta title, meta description, etc.
* Article Input & Analysis: Paste or upload your article text and optionally specify target keywords.
* AIâ€‘Driven Crossâ€‘Referencing: Using Gemini semantic analysis, the system matches your article content (and keywords) with the crawled URLs to find relevant linking opportunities.
* Link Suggestion & Insertion: Get a list of suggested internal links with anchor text recommendations. You can review, customize, and then insert them into your article (Markdown, HTML or plain text) or export the updated version.
* Flexible Tech Stack: Modern frontend (React + Vite) and robust backend (FastAPI).
* SEO & Content Workflow Optimization: Designed for content creators, bloggers & site owners to save time while boosting internal linking structure.

---

## ğŸ§± Tech Stack

| Layer       | Technology & Purpose                                                                     |
| ----------- | ---------------------------------------------------------------------------------------- |
| Frontend    | Reactâ€¯18 + Vite â€“ interactive UI for article input, link suggestions & previews.         |
| Backend     | FastAPI (Pythonâ€¯3.11+) â€“ REST APIs for crawling, AI integration & data processing.       |
| AI/ML       | Googleâ€¯Geminiâ€¯API â€“ semantic relevance matching between article content & crawled URLs.  |
| Crawling    | BeautifulSoup / Scrapy (or equivalent) â€“ discovering blog URLs, extracting metadata.     |
| Database    | SQLite (development) / PostgreSQL (production) â€“ storing crawled data and session state. |
| Other tools | Axios (frontend HTTP), Tailwind CSS (styling), Pydantic (data validation)                |

---

## ğŸ§© Prerequisites

* Node.js (v18+ recommended) and npm or yarn
* Pythonâ€¯3.11+
* A Googleâ€¯Gemini API key (obtain via Google AI Studio)
* (Optional) Docker & Docker Compose for containerised deployment

---

## ğŸš€ Installation & Setup

### Backend (FastAPI)

1. Clone the repository and change to the backend folder:

   ```bash
   git clone https://github.com/Saqib2318/Article-internal-link-builder.git
   cd Article-internal-link-builder/backend
   ```
2. Create and activate virtual environment:

   ```bash
   python -m venv venv
   source venv/bin/activate    # On Windows: venv\Scripts\activate
   ```
3. Install Python dependencies:

   ```bash
   pip install -r requirements.txt
   ```
4. Set up environment variables: create a `.env` file at the backend root with keys such as:

   ```
   GEMINI_API_KEY=your_gemini_api_key_here
   DATABASE_URL=sqlite:///./app.db     # or your PostgreSQL connection string
   ```
5. Run the FastAPI server:

   ```bash
   uvicorn main:app --reload --host 0.0.0.0 --port 8000
   ```

   The API will be accessible at [http://localhost:8000](http://localhost:8000) and API docs at [http://localhost:8000/docs](http://localhost:8000/docs).

### Frontend (React + Vite)

1. Navigate to the frontend folder:

   ```bash
   cd ../frontend
   ```
2. Install Node dependencies:

   ```bash
   npm install     # or yarn install
   ```
3. Configure the backend API base URL if needed (default is `http://localhost:8000`). You can update `src/config.js` (or environment variable) like:

   ```js
   export const API_BASE_URL = 'http://localhost:8000';
   ```
4. Run the development server:

   ```bash
   npm run dev
   ```

   The frontend app will be available at [http://localhost:5173](http://localhost:5173).

---

## ğŸ§­ Usage Guide

1. Ensure both backend and frontend servers are running.
2. In the UI, go to the â€œCrawlerâ€ tab (or equivalent):

   * Enter your website URL (e.g., `https://example.com`) or WordPress site.
   * Choose options (e.g., mark â€œWordPress modeâ€ to detect sitemap automatically).
   * Click **Start Crawl** â€“ the backend fetches blog URLs and extracts metadata.
3. Navigate to the â€œEditorâ€ or â€œArticle Inputâ€ section:

   * Paste or upload your article content.
   * (Optional) Provide target keywords (e.g., `"SEO tips"`, `"blogging strategies"`).
4. Hit **Analyze & Suggest Links** â€“ the Geminiâ€‘powered backend will process your article and match with the crawled URLs, returning link suggestions with relevance scores.
5. Review the suggestions:

   * Customize anchor texts if needed.
   * Select which links to insert into your article.
6. Insert the selected links into your article (Markdown/HTML) and/or export the updated version (Markdown, HTML, or plain text).
7. (Optional) Reâ€‘run or reâ€‘crawl whenever you publish new content and want to refresh your internal linking structure.

---

## ğŸ“ Project Structure

```
Articleâ€‘Internalâ€‘Linkâ€‘Builder/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py           # FastAPI app entry point
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â””â”€â”€ .env (template)       # Environment variable template
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ assets/       # Assets of all images scripts will used
â”‚   â”‚   â”œâ”€â”€ components/       # UI components (e.g., info-model.jsx, navbar.jsx)
â”‚   â”‚   â”œâ”€â”€ pages/         # Pages Of Web APP
â”‚   â”‚   | â”œâ”€â”€ ArticleTable.jsx         # Response Of Web App
â”‚   â”‚   | â”œâ”€â”€ FormAction.jsx           #The Page Where the user put values
â”‚   â”‚   | â”œâ”€â”€ GetStarted.jsx           # Starting Of Web APP
â”‚   â”‚   | â”œâ”€â”€ SelectAction.jsx         # Selection to go through with custom or wordpress
|   |   â”œâ”€â”€ Providers/        # Providers of Web APP
|   |   | â”œâ”€â”€selectProvider.jsx 
â”‚   â”‚   â””â”€â”€ App.jsx           # Main React component
â”‚   â”œâ”€â”€ package.json          # Node dependencies
â”‚   â”œâ”€â”€ vite.config.js        # Vite configuration
â”‚   â””â”€â”€ tailwind.config.js    # Tailwind CSS config
â”œâ”€â”€ docker-compose.yml        # (Optional) Docker setup for full stack
â””â”€â”€ README.md                 # This readme file
```

---

## âš™ï¸ Configuration & Environment Variables

* **GEMINI_API_KEY**: Your Googleâ€¯Gemini API key for semantic analysis.
* **DATABASE_URL**: URL for your database (e.g., `sqlite:///./app.db` for development, or PostgreSQL URI for production).
* **CRAWL_MAX_DEPTH**, **CRAWL_RATE_LIMIT** (example keys) â€“ configurable crawler parameters (see `backend/config.py`).
* **VITE_API_BASE_URL** or similar: For the frontend to point to backend APIs in `src/config.js` or `.env`.

---

## ğŸ¤ Contributing

We welcome contributions! Whether youâ€™re adding a feature, fixing a bug, or improving documentation:

1. Fork the repo and create a new branch:

   ```bash
   git checkout -b feature/yourâ€‘feature
   ```
2. Make your changes (please adhere to PEPâ€¯8 for Python & ESLint for JS).
3. Commit your changes and push to your branch:

   ```bash
   git commit â€‘m "Add some amazing feature"
   git push origin feature/yourâ€‘feature
   ```
4. Open a Pull Request.
5. Weâ€™ll review and merge once tests & checks are satisfied.

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€” see the `LICENSE` file for details.

---

## ğŸ™ Acknowledgements

* Huge thanks to **FastAPI** for blazingâ€‘fast, easyâ€‘toâ€‘use APIs.
* Appreciate **Vite** for the instant frontend dev feedback loop.
* Thanks to the **Googleâ€¯Gemini API** for enabling advanced semantic capabilities.
* And thank you to the openâ€‘source community for continual inspiration and excellent libraries.

---

## ğŸ’¡ Why use this tool?

* Improve your internal linking structure **automatically**, rather than doing manual linkâ€‘research one by one.
* Generate links that are **semantically relevant**â€”not just keyword matches, but contextually meaningfulâ€”thanks to AI assistance.
* Save time and energy: Let the crawler go fetch metadata, let the AI find relevant anchorâ€‘texts, and you just review and export.
* Especially useful for blogs with **lots of content** (WordPress or custom) where building internal links manually becomes tedious and errorâ€‘prone.
* Helps boost **SEO**, improves user experience (via better navigation), and strengthens content discoverability.

---

## ğŸ”® Future Roadmap (ideas)

* Support bulk upload of multiple articles at once.
* Add userâ€‘accounts / login to manage multiple websites.
* Dashboard with crawl analytics (link performance, internal link counts, crawl history).
* Integration with CMS platforms (WordPress plugin, Ghost, etc.).
* More AIâ€‘powered suggestions: e.g., suggest new blog topics, identify orphan pages etc.
* Multiâ€‘language support for international sites.

---

Again, feel free to **edit**, **trim**, or **expand** this README to match your exact implementation or feature set. If you want a simpler version (for nonâ€‘technical users) or a more detailed developer version (with API specs, architecture diagrams) I can help with those too.
